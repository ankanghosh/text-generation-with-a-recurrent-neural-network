{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankanghosh/text-generation-with-a-recurrent-neural-network/blob/main/Text_Generation_Using_An_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text Generation using an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "DGQffjwbjZ5B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based Recurrent Neural Network (RNN). The tutorial is based on TensorFlow's tutorial about [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation) and takes motivation from Andrej Karpathy's blog about [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and computing resources"
      ],
      "metadata": {
        "id": "pYw3jJeUjce1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this work, we will use the [Cornell Movie-Dialogs Corpus](https://convokit.cornell.edu/documentation/movie.html) dataset -  a collection of fictional conversations extracted from raw movie scripts. The objective is to train a model to predict the next character in a sequence based on a given sequence of characters extracted from the dialogue data. Invoking the model repeatedly helps generate longer text sequences.\n",
        "\n",
        "We will require a considerable amount of computing power or RAM for processing the data (text tokenization) and for fitting the final RNN. This can be managed with careful use of the TPUs or GPUs allotted under the free tier of Colab. However, if you do not enjoy usage restrictions, you could also consider [upgrading](https://colab.research.google.com/signup) to *Colab Pro* or go for *Pay As you Go*, as we will need to use a GPU or a TPU to process the data and to build the model.\n",
        "\n",
        "We will be using a TPU in this work, as it is relatively cheaper than the GPUs available in the Colab environment and also because we would be leveraging TensorFlow's distributed computing capacity to optimize the training time and resources. This can be achieved by navigating as follows.\n",
        "*   Go to Runtime --> Change runtime type -->\n",
        "Select TPU v2-8 under Hardware accelerator.\n",
        "\n",
        "We can always monitor the usage of the resources we have selected as follows.\n",
        "*   Go to Runtime --> View resources."
      ],
      "metadata": {
        "id": "JPKjj1EJjhTi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "Install the ConvoKit library to download the [Cornell Movie-Dialogs Corpus](https://convokit.cornell.edu/documentation/movie.html). This library uninstalls the existing `NumPy 1.x` and downloads and installs `NumPy 2.x`. This results in errors while importing TensorFlow. We will thereforefore go ahead and uninstall the NumPy version installed by ConvoKit and reinstall the original version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AlFD4IMGRmTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0adffb-9433-4a4f-c12a-cb522fdfdc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.1.tar.gz (187 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/188.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m184.3/188.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.2.2)\n",
            "Collecting numpy>=2.0.0 (from convokit)\n",
            "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting spacy>=3.8.2 (from convokit)\n",
            "  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.9.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.6)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from convokit)\n",
            "  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.12.1)\n",
            "Collecting numexpr>=2.8.0 (from convokit)\n",
            "  Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting ruff>=0.4.8 (from convokit)\n",
            "  Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting bottleneck (from convokit)\n",
            "  Downloading Bottleneck-1.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting matplotlib>=3.0.0 (from convokit)\n",
            "  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.9.0.post0)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->convokit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->convokit) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->convokit) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (0.15.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.5.0)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->convokit)\n",
            "  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->convokit) (0.1.5)\n",
            "Collecting numpy>=2.0.0 (from convokit)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2024.8.30)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.8.2->convokit) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (0.1.2)\n",
            "Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.3/397.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Bottleneck-1.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (356 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.1/356.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.1-py3-none-any.whl size=221408 sha256=9b74f2b10e37992b1f84534b9e92abf217c8fc33ca31e58f5e1083b3098e7032\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/45/5a/e41ab3344b5f606d8f387c109b15c24732754e3c5ec4e069f2\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=db434f0821b8cc8e315be191d8d84d5e8a3d031c7f6545b8374255e9ad7fef1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, ruff, numpy, ftfy, dnspython, dill, pymongo, numexpr, msgpack-numpy, clean-text, bottleneck, blis, matplotlib, thinc, spacy, convokit\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.0.1 bottleneck-1.4.2 clean-text-0.6.0 convokit-3.0.1 dill-0.3.9 dnspython-2.7.0 emoji-1.7.0 ftfy-6.3.1 matplotlib-3.9.3 msgpack-numpy-0.4.8 numexpr-2.10.2 numpy-2.0.2 pymongo-4.10.1 ruff-0.8.2 spacy-3.8.2 thinc-8.3.2 unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4F1HsoDeZQC",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed472acd-d1d2-4f6e-d558-122ae2951d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "convokit 3.0.1 requires numpy>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries and modules needed for fetching and processing the data, and other ML tasks."
      ],
      "metadata": {
        "id": "UjhekJpgxGyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "from convokit import Corpus, download\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch and read the data"
      ],
      "metadata": {
        "id": "4hrRIJo_QfAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing ConvoKit will download the Cornell Movie-Dialogs Corpus dataset. We are interested in the file `utterances.jsonl` file that gets saved in the local Colab environment at `/root/.convokit/saved-corpora/movie-corpus/`. Each line in this JSON Lines (JSONL) file corresponds to an utterance or dialogue (with its ID represented by an index by the field `id`) from a movie, tagged with a `conversation_id` (ID of the first utterance in the conversation this utterance belongs to) and other information pertaining to the dialogue. The utterance or dialogue itself is represented by the field `text`. More [here](https://convokit.cornell.edu/documentation/movie.html). Viewing the summary of the downloaded dataset reveals that there are `304713` lines and `83097` conversations in the dataset.\n",
        "\n",
        "**Note:** Generally, dialogues can refer to a single utterance or a combination of utterances forming a conversation. For simplicity, we will refer to each utterance as a \"dialogue\" and use the term \"conversation\" for a series / combination of utterances / dialogues."
      ],
      "metadata": {
        "id": "Sx5FWOzboW-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the data"
      ],
      "metadata": {
        "id": "8Rq7BTPQQQ31"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko2uBritRujg",
        "outputId": "ffc4dda6-df3b-40e6-88ce-6cdb3773fe98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "model_directory: ~/.convokit/saved-models\n",
            "default_backend: mem\n",
            "Downloading movie-corpus to /root/.convokit/saved-corpora/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
            "Number of Speakers: 9035\n",
            "Number of Utterances: 304713\n",
            "Number of Conversations: 83097\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset.\n",
        "corpus_path = download(\"movie-corpus\")\n",
        "corpus = Corpus(filename=corpus_path)\n",
        "\n",
        "# Print a summary of statistics related to the corpus.\n",
        "corpus.print_summary_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg-mg2OYbDcm"
      },
      "outputs": [],
      "source": [
        "# Path to \"utterances.jsonl\".\n",
        "utterances_path = os.path.join(corpus_path, \"utterances.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "View some of the utterances for inspection."
      ],
      "metadata": {
        "id": "ZufSB83J1jhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of lines to read.\n",
        "num_lines = 10\n",
        "\n",
        "# Open the JSONL file and read the first \"num_lines\" lines.\n",
        "with open(utterances_path, \"r\") as file:\n",
        "    first_num_lines = [json.loads(next(file)) for _ in range(num_lines)]\n",
        "\n",
        "# Convert the JSONL lines to a DataFrame.\n",
        "first_num_lines_utterances = pd.DataFrame(first_num_lines)\n",
        "\n",
        "# Display the first few rows of utterances.\n",
        "first_num_lines_utterances.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JP3dkOudGRTO",
        "outputId": "cee59d94-afd8-4bb5-d7e7-531ab0b49993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id conversation_id          text speaker  \\\n",
              "0  L1045           L1044  They do not!      u0   \n",
              "1  L1044           L1044   They do to!      u2   \n",
              "2   L985            L984    I hope so.      u0   \n",
              "3   L984            L984     She okay?      u2   \n",
              "4   L925            L924     Let's go.      u0   \n",
              "\n",
              "                                                meta reply-to timestamp  \\\n",
              "0  {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...    L1044      None   \n",
              "1  {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...     None      None   \n",
              "2  {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...     L984      None   \n",
              "3  {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...     None      None   \n",
              "4  {'movie_id': 'm0', 'parsed': [{'rt': 0, 'toks'...     L924      None   \n",
              "\n",
              "  vectors  \n",
              "0      []  \n",
              "1      []  \n",
              "2      []  \n",
              "3      []  \n",
              "4      []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc2f5e3a-50ba-45d8-b661-21274798ee03\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>text</th>\n",
              "      <th>speaker</th>\n",
              "      <th>meta</th>\n",
              "      <th>reply-to</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>L1045</td>\n",
              "      <td>L1044</td>\n",
              "      <td>They do not!</td>\n",
              "      <td>u0</td>\n",
              "      <td>{'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...</td>\n",
              "      <td>L1044</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L1044</td>\n",
              "      <td>L1044</td>\n",
              "      <td>They do to!</td>\n",
              "      <td>u2</td>\n",
              "      <td>{'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L985</td>\n",
              "      <td>L984</td>\n",
              "      <td>I hope so.</td>\n",
              "      <td>u0</td>\n",
              "      <td>{'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...</td>\n",
              "      <td>L984</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>L984</td>\n",
              "      <td>L984</td>\n",
              "      <td>She okay?</td>\n",
              "      <td>u2</td>\n",
              "      <td>{'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks'...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L925</td>\n",
              "      <td>L924</td>\n",
              "      <td>Let's go.</td>\n",
              "      <td>u0</td>\n",
              "      <td>{'movie_id': 'm0', 'parsed': [{'rt': 0, 'toks'...</td>\n",
              "      <td>L924</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc2f5e3a-50ba-45d8-b661-21274798ee03')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc2f5e3a-50ba-45d8-b661-21274798ee03 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc2f5e3a-50ba-45d8-b661-21274798ee03');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02842c37-4157-4717-a235-9a17429e165f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02842c37-4157-4717-a235-9a17429e165f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02842c37-4157-4717-a235-9a17429e165f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "first_num_lines_utterances",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the dataset"
      ],
      "metadata": {
        "id": "Hc7urqWPRGee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the utterances from `utterances.jsonl`, map them by utterance ID, and group them by `conversation_id` to create a dictionary of conversations. The goal is to group the utterances or dialogues belonging to a particular conversation from a movie."
      ],
      "metadata": {
        "id": "jq4Jw6gQ-EMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuHGqSilfhxN"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load utterances and build a map of \"utterance_id -> utterance\".\n",
        "utterance_map = {}\n",
        "\n",
        "# To store the conversations with their utterances.\n",
        "conversation_map = {}\n",
        "\n",
        "# Open the \"utterances.jsonl\" file for reading.\n",
        "with open(utterances_path, \"r\") as file:\n",
        "    # Iterate over each line in the file.\n",
        "    for line in file:\n",
        "        # Parse the JSON object from the line into a Python dictionary (utterance).\n",
        "        utterance = json.loads(line)\n",
        "\n",
        "        # Store the utterance in the \"utterance_map\" dictionary using the utterance's ID as the key.\n",
        "        utterance_map[utterance[\"id\"]] = utterance\n",
        "\n",
        "        # Retrieve the \"conversation_id\" from the utterance to group it by conversation.\n",
        "        conversation_id = utterance.get(\"conversation_id\")\n",
        "\n",
        "        # If the \"conversation_id\" exists in the utterance:\n",
        "        if conversation_id:\n",
        "            # Check if this conversation already has an entry in the \"conversation_map\".\n",
        "            if conversation_id not in conversation_map:\n",
        "                # If not, create a new list to store the utterance IDs for this conversation.\n",
        "                conversation_map[conversation_id] = []\n",
        "\n",
        "            # Append the current utterance's ID to the list of utterances for the conversation.\n",
        "            conversation_map[conversation_id].append(utterance['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the first 5 conversation IDs and their corresponding utterance IDs."
      ],
      "metadata": {
        "id": "0f6WgY_f-223"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 conversation IDs and their corresponding utterance IDs.\n",
        "print(\"First 5 elements in conversation_map:\")\n",
        "for i, (conversation_id, utterance_ids) in enumerate(conversation_map.items()):\n",
        "    print(f\"Conversation id: {conversation_id}, Utterance ids: {utterance_ids}\")\n",
        "    # Stop after the first 5 elements.\n",
        "    if i >= 4:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27X6gjnFqPUL",
        "outputId": "765648c7-bd55-4754-c52f-f07b3b29359c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 elements in conversation_map:\n",
            "Conversation id: L1044, Utterance ids: ['L1045', 'L1044']\n",
            "Conversation id: L984, Utterance ids: ['L985', 'L984']\n",
            "Conversation id: L924, Utterance ids: ['L925', 'L924']\n",
            "Conversation id: L870, Utterance ids: ['L872', 'L871', 'L870']\n",
            "Conversation id: L866, Utterance ids: ['L869', 'L868', 'L867', 'L866']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the utterance IDs are grouped in sequences of decreasing numbers following the alphabet \"L\", e.g., `L872`, `L871`, and `L870`. This pattern holds for the entire dataset and reflects how it is structured. To group the utterances or dialogues from a particular conversation in a movie in a way that forms a coherent conversation, we will combine the texts (dialogues) corresponding to the utterance IDs in reverse order, i.e., starting from `L870`, followed by `L871`, and then `L872`. We will review a few examples of the resulting conversations shortly, however, this can be confirmed by investigating the data in the `utterances.jsonl` file. Store the text conversations in a separate output file called `conversations.txt` and confirm that there are `83097` conversations in it."
      ],
      "metadata": {
        "id": "3JM7KHahIvnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output file path.\n",
        "output_file_path = \"conversations.txt\"\n",
        "\n",
        "# Open the output file for writing.\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    # Iterate over each conversation in the \"conversation_map\".\n",
        "    for conversation_id, utterance_ids in conversation_map.items():\n",
        "        # Reverse the list of utterance IDs to process them in increasing order of \"L\" numbers.\n",
        "        reversed_ids = reversed(utterance_ids)\n",
        "\n",
        "        # Collect the texts corresponding to each utterance ID in the reversed order.\n",
        "        conversation_text = []\n",
        "        for utterance_id in reversed_ids:\n",
        "            # Fetch the utterance text or dialogue from the \"utterance_map\".\n",
        "            utterance_text = utterance_map[utterance_id][\"text\"]\n",
        "\n",
        "            # Add the text of the current utterance to the \"conversation_text\" list.\n",
        "            # This accumulates the dialogue in the reversed order for the conversation.\n",
        "            conversation_text.append(utterance_text)\n",
        "\n",
        "        # Join the collected texts with newlines to form the full conversation.\n",
        "        full_conversation = \"\\n\".join(conversation_text)\n",
        "\n",
        "        # Write the full conversation to the output file, followed by an extra blank line for separation.\n",
        "        output_file.write(full_conversation + \"\\n\\n\")\n",
        "\n",
        "print(f\"Conversations dataset created with {len(conversation_map)} conversations and saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCZbv8dfy_JE",
        "outputId": "743fe3d0-3b45-4ba8-edde-352ff8140c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations dataset created with 83097 conversations and saved to conversations.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the first 5 conversations from the `conversations.txt` file ."
      ],
      "metadata": {
        "id": "8GdyKt_mY8wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the \"conversations.txt\" file and print the first 5 conversations.\n",
        "with open(output_file_path, 'r') as file:\n",
        "    # Split the content based on two consecutive newline characters (\"\\n\\n\").\n",
        "    conversations = file.read().strip().split(\"\\n\\n\")\n",
        "    print(\"\\nFirst 5 conversations from conversations.txt:\")\n",
        "\n",
        "    # Loop through and print the first 5 conversations.\n",
        "    for i, conversation in enumerate(conversations[:5]):\n",
        "        print(f\"Conversation {i + 1}:\\n{conversation}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MTtaoFnqSqT",
        "outputId": "9015c535-3355-46c2-8beb-36d5abec87ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 conversations from conversations.txt:\n",
            "Conversation 1:\n",
            "They do to!\n",
            "They do not!\n",
            "\n",
            "Conversation 2:\n",
            "She okay?\n",
            "I hope so.\n",
            "\n",
            "Conversation 3:\n",
            "Wow\n",
            "Let's go.\n",
            "\n",
            "Conversation 4:\n",
            "I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "No\n",
            "Okay -- you're gonna need to learn how to lie.\n",
            "\n",
            "Conversation 5:\n",
            "I figured you'd get to the good stuff eventually.\n",
            "What good stuff?\n",
            "The \"real you\".\n",
            "Like my fear of wearing pastels?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the text data in the `conversations.txt` file at the character-level. We will refer to the entire text content in the file as “text” in this tutorial."
      ],
      "metadata": {
        "id": "rqDaR2ePU66h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the output file and decode it using \"utf-8\" (compatible with Python 2 and 3).\n",
        "text = open(output_file_path, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "\n",
        "# Length of the text is the number of characters in it.\n",
        "print(f\"Length of the text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_DZRf2rtvUl",
        "outputId": "b901141e-793d-4abc-b85b-081b7acfea51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the text: 17226110 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine the first 500 characters to get a sense of the text content."
      ],
      "metadata": {
        "id": "iPYUWOsjVBjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd0a107-b2c3-413d-ee3c-a9d7bb1f8f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They do to!\n",
            "They do not!\n",
            "\n",
            "She okay?\n",
            "I hope so.\n",
            "\n",
            "Wow\n",
            "Let's go.\n",
            "\n",
            "I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "No\n",
            "Okay -- you're gonna need to learn how to lie.\n",
            "\n",
            "I figured you'd get to the good stuff eventually.\n",
            "What good stuff?\n",
            "The \"real you\".\n",
            "Like my fear of wearing pastels?\n",
            "\n",
            "do you listen to this crap?\n",
            "What crap?\n",
            "Me.  This endless ...blonde babble. I'm like, boring myself.\n",
            "Thank God!  If I had to hear one more story about your coiffure...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Examine the first 500 characters to get a sense of the text content.\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a sorted list of all unique characters in the text. This represents the vocabulary of the dataset. Find out how many unique characters are present in the text."
      ],
      "metadata": {
        "id": "7Ql8QIq4V1Pn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed707cfc-14a1-4d24-ba36-860d644dc5a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Create a sorted list of all unique characters in the text.\n",
        "# This represents the vocabulary of the dataset.\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Print the number of unique characters in the vocabulary.\n",
        "print(f\"{len(vocab)} unique characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the text data before it is fed into the RNN-based sequence model. This involves constructing a data pipeline to streamline the flow of data through stages, such as text vectorization, splitting the vectorized data into sequences, creating input-target pairs for training, efficient data loading, and training optimization to help improve the performance of the data pipeline."
      ],
      "metadata": {
        "id": "jHGSJKaOxI4K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text needs to be split into tokens and then vectorized. Tokens are smaller units of text, such as characters, words, or subwords - characters in this case. Vectorization is the process of converting the text strings to a numerical representation. The `tf.keras.layers.StringLookup` layer maps each character (token) in the text into a numeric ID. See example texts and character-level tokenization below."
      ],
      "metadata": {
        "id": "Kp3xNZwViV6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d08208-f338-4a69-b5fd-8fbec729ddae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Example list of text strings.\n",
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# Split the strings into individual characters.\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "\n",
        "# Display the tensor containing the split characters.\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the prefix `b` before each character (e.g., `b'a'`) indicates that the data is being stored as a byte string."
      ],
      "metadata": {
        "id": "MwexJZd7yvA4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Create a `StringLookup` layer to convert characters to numeric IDs using the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "# Create a \"StringLookup\" layer to convert characters to numeric IDs using the vocabulary.\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "Convert tokens (characters) to their numeric character IDs and view the IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2515de0e-5dbb-478b-c06c-dbe454093455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[65, 66, 67, 68, 69, 70, 71], [88, 89, 90]]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Convert characters to their corresponding numeric IDs.\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "# Display the tensor containing the numeric IDs.\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "To reverse the tokenization process and recover human-readable strings, use the `StringLookup` layer with `invert=True`. This will map the numeric IDs back to their corresponding characters. Additionally, instead of using the original vocabulary generated by `sorted(set(text))`, use the `get_vocabulary()` method of the `StringLookup` layer to ensure that the `[UNK]` or \"unknown\" token is handled consistently when preprocessing the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "# Create a \"StringLookup\" layer to map token IDs back to characters (invert the original mapping).\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "Retrieve the tensor containing the characters from the vectors of numeric IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204c8d34-89e2-4f33-cce2-1f9ffec363e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Convert token IDs back to characters using the inverse mapping.\n",
        "chars = chars_from_ids(ids)\n",
        "\n",
        "# Display the tensor containing the characters.\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Join the characters back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1340f0-c437-4abb-ee1f-6af2ee638d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Join the characters along the last axis to reconstruct the text string.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to convert token IDs back into text by mapping the IDs to characters and joining them."
      ],
      "metadata": {
        "id": "lLM9QKLez-10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "# Converts token IDs back into text by mapping the IDs to characters and joining them.\n",
        "def text_from_ids(ids):\n",
        "  # Converts token IDs back into the corresponding text string by joining characters.\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "The goal of this work is to create an RNN and train it to perform character-level prediction for text generation. RNNs are a class of neural networks designed for processing sequential data, such as time series, natural language, or any data where the order of the input matters. RNNs have loops that allow information to persist in the network, enabling them to handle sequences of varying lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RNN-based model will have a character or a sequence of characters as its input and will be trained to predict / output the following character (most probable character) at each time step. RNNs maintain an internal state or hidden state that embodies information representing a compressed summary of all the inputs it has processed up to a given time step. When processing the current element, it integrates the influence of the current input on the state with the previous internal state. The internal state of the RNN encapsulates long-term dependencies and acts like a dynamic memory that helps predict the next character, given all the characters computed until this moment."
      ],
      "metadata": {
        "id": "FMKBarW6Sj25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In this tutorial, we will be using Gated Recurrent Units (GRUs), a more efficient variant of the RNN designed to address vanishing gradient issues. GRUs integrate the influence of the current input with the previous internal / hidden state using specialized mechanisms like the reset and update gates. These gates ensure that irrelevant past information can be forgotten and relevant new information is retained."
      ],
      "metadata": {
        "id": "WbAD3-jzSnr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets"
      ],
      "metadata": {
        "id": "bPZzRT8dDOed"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "Next, divide the text into example sequences, where each input sequence contains `seq_length` characters from the text. For every input sequence, the corresponding target sequence will consist of the same length of text but will be shifted by one character to the right. This shift happens because the target sequence starts from the next character after the first input character, which is what the RNN is tasked with predicting - the next character. These right-shifted characters in the target sequence help the model learn to predict the next character in the sequence. For instance, if `seq_length` is `4` and the text is \"Hello\", the input sequence would be \"Hell\" and the target sequence would be \"ello\". To implement this:\n",
        "\n",
        "1.   Use `tf.data.Dataset.from_tensor_slices` to convert the text into a TensorFlow dataset consisting of a stream of character indices.\n",
        "2.   Break the text into chunks of `seq_length+1`. This ensures that the input sequence excludes the last character in the chunk, allowing it to match the length of the target sequence, which is right-shifted by one character."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go ahead and convert all of the characters in the text into their corresponding numeric IDs and review the resulting tensor."
      ],
      "metadata": {
        "id": "M3XRcv_HUooS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c961c3-164a-45de-cabd-28eb5b16b76b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(17226110,), dtype=int64, numpy=array([53, 72, 69, ..., 84,  2,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Convert all of the characters in the text into their corresponding numeric IDs.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "\n",
        "# Display the tensor containing the numeric IDs.\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us review an example to convert a few of the token IDs back to their corresponding characters and generate the text."
      ],
      "metadata": {
        "id": "2f2_DSNgMUe6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFiKkm5EzxzU",
        "outputId": "7b85e312-fee1-4aaa-befd-770d24df887e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: They do to!\n",
            "They do not!\n",
            "\n",
            "She okay?\n",
            "I hope so.\n",
            "\n",
            "Wow\n",
            "Let's go.\n",
            "\n",
            "I'm kidding.  You know how sometimes \n"
          ]
        }
      ],
      "source": [
        "# Convert the first 100 token IDs back to characters and generate the text.\n",
        "generated_text = text_from_ids(all_ids[:100])\n",
        "\n",
        "# Print the generated text as a UTF-8 decoded string.\n",
        "print(\"Generated Text:\", generated_text.numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a TensorFlow dataset from the list of all token IDs corresponding to the text data."
      ],
      "metadata": {
        "id": "HZATd5H-QTR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "# Create a TensorFlow dataset from the list of all token IDs corresponding to the text data.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first 10 text sequences by converting the token IDs back to characters."
      ],
      "metadata": {
        "id": "wFOwtTszQqQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjH5v45-yqqH",
        "outputId": "93e32851-3f3d-4d45-9f20-457111d1bb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n",
            "h\n",
            "e\n",
            "y\n",
            " \n",
            "d\n",
            "o\n",
            " \n",
            "t\n",
            "o\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 text sequences by converting the token IDs back to characters.\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the sequence length for the input sequences to be fed to the RNN-based model."
      ],
      "metadata": {
        "id": "CUTKGUwBQtB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "# Define the sequence length for the input sequences to be fed to the RNN-based model.\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch()` method helps convert individual characters to sequences of the desired size. Go ahead and batch sequences of length `seq_length+1` from the TensorFlow dataset of list of all token IDs and print the first batch of sequences as characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpdjRO2CzOfZ",
        "outputId": "c39d4149-a130-4326-aade-1bfa8fbcecc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'T' b'h' b'e' b'y' b' ' b'd' b'o' b' ' b't' b'o' b'!' b'\\n' b'T' b'h'\n",
            " b'e' b'y' b' ' b'd' b'o' b' ' b'n' b'o' b't' b'!' b'\\n' b'\\n' b'S' b'h'\n",
            " b'e' b' ' b'o' b'k' b'a' b'y' b'?' b'\\n' b'I' b' ' b'h' b'o' b'p' b'e'\n",
            " b' ' b's' b'o' b'.' b'\\n' b'\\n' b'W' b'o' b'w' b'\\n' b'L' b'e' b't' b\"'\"\n",
            " b's' b' ' b'g' b'o' b'.' b'\\n' b'\\n' b'I' b\"'\" b'm' b' ' b'k' b'i' b'd'\n",
            " b'd' b'i' b'n' b'g' b'.' b' ' b' ' b'Y' b'o' b'u' b' ' b'k' b'n' b'o'\n",
            " b'w' b' ' b'h' b'o' b'w' b' ' b's' b'o' b'm' b'e' b't' b'i' b'm' b'e'\n",
            " b's' b' ' b'y'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Batch sequences of length \"seq_length+1\" from the TensorFlow dataset of list of all token IDs.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Print the tensor comprising of the first batch of sequences as characters.\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "Join the tokens back into strings to see what this is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO32cMWu4a06",
        "outputId": "6d17ecaf-30f4-4ca5-a989-592ff9dfd78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"They do to!\\nThey do not!\\n\\nShe okay?\\nI hope so.\\n\\nWow\\nLet's go.\\n\\nI'm kidding.  You know how sometimes y\"\n",
            "b'ou just become this \"persona\"?  And you don\\'t know how to quit?\\nNo\\nOkay -- you\\'re gonna need to learn'\n",
            "b' how to lie.\\n\\nI figured you\\'d get to the good stuff eventually.\\nWhat good stuff?\\nThe \"real you\".\\nLike'\n",
            "b' my fear of wearing pastels?\\n\\ndo you listen to this crap?\\nWhat crap?\\nMe.  This endless ...blonde babb'\n",
            "b\"le. I'm like, boring myself.\\nThank God!  If I had to hear one more story about your coiffure...\\n\\nThen\"\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the first 5 batches of sequences and print the corresponding generated text.\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Training will require a dataset of `(input, label)` pairs, where `input` and\n",
        "`label` are sequences. At each time step, the input is the current character and the label is the next character.\n",
        "\n",
        "Define a function that accepts a sequence and splits it into input and target sequences by shifting the input sequence one character to the right, aligning the input and target labels for each time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "# Split a sequence into input and target sequences by shifting the input sequence one character to the right.\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]  # Input sequence: all characters except the last.\n",
        "    target_text = sequence[1:]  # Target sequence: all characters except the first (shifted).\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the `split_input_target()` function with a sample text."
      ],
      "metadata": {
        "id": "phO-O6Gle8gw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxbDTJTw5u_P",
        "outputId": "bf157272-2b65-4004-f221-cd8443bfe833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Split the sequence \"Tensorflow\" into input and target sequences.\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the `split_input_target()` function to each sequence in the dataset to create input and target pairs."
      ],
      "metadata": {
        "id": "EQ5MyEmngayB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "# Apply the \"split_input_target()\" function to each sequence in the dataset to create input and target pairs.\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate through the dataset and print the first input-target pair."
      ],
      "metadata": {
        "id": "qixyDCXvisI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "29c69482-0938-463c-8f2f-5a8b128b9605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"They do to!\\nThey do not!\\n\\nShe okay?\\nI hope so.\\n\\nWow\\nLet's go.\\n\\nI'm kidding.  You know how sometimes \"\n",
            "Target: b\"hey do to!\\nThey do not!\\n\\nShe okay?\\nI hope so.\\n\\nWow\\nLet's go.\\n\\nI'm kidding.  You know how sometimes y\"\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the dataset and print the first input-target pair.\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    # Convert the input sequence from token IDs to characters and print it.\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "\n",
        "    # Convert the target sequence from token IDs to characters and print it.\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tf.data` has been used to split the text into manageable sequences. However, the data is required to be shuffled and packed into batches before being fed to the RNN-based sequence model.\n",
        "\n",
        "There are two important methods that should be used when loading data to make sure that I/O does not become blocking.\n",
        "1.   `cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training the model.\n",
        "2.   `prefetch()` overlaps data preprocessing and model execution while training.\n",
        "\n",
        "More on both of the aforementioned methods, as well as how to cache data to disk in the data performance guide [here](https://www.tensorflow.org/guide/data_performance).\n",
        "\n",
        "The `shuffle()` method in the data pipeline randomizes the order of the elements in the dataset. This helps prevent overfitting of the data and improves the generalization capabilities of the model. `batch()` is responsible for grouping the data into batches.\n",
        "\n",
        "Define the batch size and the buffer size for training the model. Use `shuffle()` and `batch()` to load the data and use training optimization techniques (`cache()` and `prefetch()`) to help improve the performance of the data pipeline."
      ],
      "metadata": {
        "id": "dqdZYt5fjs1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "f742d6ab-45fe-484d-f19b-60a72bcb02b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(256, 100), dtype=tf.int64, name=None), TensorSpec(shape=(256, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Automatically tune the buffer size for optimal data loading performance.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Define the batch size.\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Define the buffer size to shuffle the dataset\n",
        "# (\"tf.data\" is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Cache, shuffle, batch, and prefetch the text data to improve performance.\n",
        "dataset = (\n",
        "    dataset\n",
        "    .cache()  # Cache the text data in memory or a local file.\n",
        "    .shuffle(BUFFER_SIZE)  # Shuffle the dataset to ensure input data is randomized.\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)  # Group data into BATCH_SIZE batches, discarding incomplete ones.\n",
        "    .prefetch(tf.data.AUTOTUNE)  # Use tf.data.AUTOTUNE for optimized prefetching.\n",
        ")\n",
        "\n",
        "# Inspect the dataset pipeline for correctness.\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This work will use an RNN to build the model, specifically, it will use a GRU. Let us go ahead and define the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model"
      ],
      "metadata": {
        "id": "jB2n1NcDYn8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the size of the vocabulary in the `StringLookup` layer, embedding dimension, and the number of RNN units (or GRU units) in the `GRU` layer."
      ],
      "metadata": {
        "id": "f7YlHoj-6VTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Define the size of the vocabulary in the \"StringLookup\" layer.\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# Define the embedding dimension.\n",
        "embedding_dim = 256\n",
        "\n",
        "# Define the number of RNN / GRU units.\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has three layers discussed below.\n",
        "\n",
        "1.   `tf.keras.layers.Embedding`: The input layer - a trainable lookup table that maps each character or token ID to a vector with `embedding_dim` dimensions.\n",
        "2.   `tf.keras.layers.GRU`: A GRU with size `units=rnn_units` that processes sequences, outputs predictions for each time step (full sequences), and retains a final state for continued predictions.\n",
        "3.   `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit (raw, unnormalized output of the model) for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ],
      "metadata": {
        "id": "Jl1wC54GI1Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The `call()` method defines the forward pass by embedding the input, initializing the internal states if necessary, processing sequences with the `GRU` layer, and generating character predictions via the `Dense` layer. It also allows for dynamic batch handling and returns the updated internal states for continued text generation."
      ],
      "metadata": {
        "id": "wqnb6HGYMW2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class RNNModel(tf.keras.Model):\n",
        "    # Custom RNN-based model using a GRU with an embedding and a dense layer.\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "\n",
        "        # \"Embedding\" layer: Maps token IDs to dense vectors of size \"embedding_dim\".\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # \"GRU\" layer: Processes sequences, returns full sequences and final state.\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,  # Return sequence at each time step.\n",
        "            return_state=True       # Return the final state.\n",
        "        )\n",
        "        # \"Dense\" layer: Projects the GRU output to \"vocab_size\" for predictions.\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # Save the number of RNN units.\n",
        "        self.rnn_units = rnn_units\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        # Forward pass logic.\n",
        "\n",
        "        # Input data.\n",
        "        x = inputs\n",
        "\n",
        "        # \"Embedding\" layer: Transform inputs to dense vectors.\n",
        "        x = self.embedding(inputs, training=training)\n",
        "\n",
        "        # Initialize states if not provided.\n",
        "        if states is None:\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "            # Conditionally handle the case when the batch size is not known.\n",
        "            states = tf.cond(\n",
        "                tf.equal(batch_size, tf.constant(0, dtype=tf.int32)),  # Case when the batch size is 0.\n",
        "                lambda: tf.zeros((1, self.rnn_units), dtype=tf.float32),  # Default to a batch size of 1.\n",
        "                lambda: tf.zeros((batch_size, self.rnn_units), dtype=tf.float32)  # Dynamic batch size.\n",
        "            )\n",
        "\n",
        "        # \"GRU\" layer: Process sequence and update states.\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "        # \"Dense\" layer: Predict the next character probabilities.\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        # Return both predictions and states if required.\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a model to test"
      ],
      "metadata": {
        "id": "p1fQhG1SUO3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model using a specified vocabulary size, embedding dimensionality, and the number of RNN units in the `GRU` layer. To investigate the characteristics and behavior of the RNN-based model, create a test model. The original / final model will utilize the distributed training infrastructure provided by the TPU, requiring its initialization and compilation within the scope of the TPU’s distributed training strategy. Before proceeding with training, it is beneficial to test the model to ensure it behaves as expected.\n",
        "\n",
        "**Note:** For simplicity, we will refer to the test model as \"model\" when discussing aspects pertaining to the `RNNModel` in general."
      ],
      "metadata": {
        "id": "raB4xhXsAjTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "# Initialize the \"RNNModel\" with specified hyperparameters.\n",
        "test_model = RNNModel(\n",
        "    vocab_size=vocab_size,          # Size of the vocabulary (number of unique characters).\n",
        "    embedding_dim=embedding_dim,    # Dimensionality of the \"Embedding\" layer.\n",
        "    rnn_units=rnn_units             # Number of RNN units in the \"GRU\" layer.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character, the model looks up its embedding, processes it through a GRU unit for one time step, and then passes the output through a dense layer to generate logits that predict the log-likelihood of the next character. See image below that describes how the data passes through the model in general.\n",
        "\n",
        "**Note:** The image has been sourced from the original TensorFlow tutorial about [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "### Try the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the test model on one batch of input-output pairs from the dataset to verify that it behaves as expected. Check the shape of the output to start with."
      ],
      "metadata": {
        "id": "YRgc2KBtM175"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "c348c2fe-6177-494f-ce0b-42c433ba46eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 100, 95) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the first batch of input-output pairs from the dataset.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    # Generate predictions for the input batch by passing it through the model.\n",
        "    example_batch_predictions = test_model(input_example_batch)\n",
        "\n",
        "    # Print the shape of the output predictions.\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Since `dataset.take(1)` retrieves only a single batch (the first batch) of data, the unpacked variables `input_example_batch`, `target_example_batch`, and `example_batch_predictions` remain accessible outside the `for` loop. These variables will be used later in the code."
      ],
      "metadata": {
        "id": "NpcpZxrxmfDC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "Investigate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "7590f072-e32c-4c56-e5a8-f7d02cc8a498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"rnn_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24320     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  97375     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4059999 (15.49 MB)\n",
            "Trainable params: 4059999 (15.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Investigate the model.\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model, sample from the output distribution instead of simply taking the *argmax* of the logits and selecting the character with the highest probability (the “most likely” next character). The logits define a probability distribution over the character vocabulary for the next character. Sampling from this distribution helps select an actual character index in a way that introduces randomness into the generation process. This randomness is crucial because always using *argmax* can cause the model to get stuck in repetitive loops, continually predicting the same character. By sampling, the model generates different outputs each time, even when starting from the same context.\n",
        "\n",
        "For the first example in the first batch of data, sample indices from the model’s predicted probability distribution to select the next character at each time step. Remove unnecessary dimensions and convert the result for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "# Sample indices from the predicted probability distribution.\n",
        "# This selects the next character based on the model's logits for the current time step.\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "\n",
        "# Remove extra dimensions of size 1 from the sampled indices and\n",
        "# convert the tensor to a NumPy array for easier manipulation.\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "View the prediction of the next character index at each time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqFMUQc_UFgM",
        "outputId": "a9b86770-8bf8-446c-86bb-597a38b98ee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 65, 50, 82, 22, 71, 81, 59, 19, 86, 89,  1, 92, 44, 49, 11, 43,\n",
              "       20, 23, 32, 51, 27, 92, 10, 68, 62, 10, 18, 68, 48,  2, 53, 13, 46,\n",
              "       49, 15, 47, 30, 84, 82,  2, 33, 86, 41, 19, 79, 29, 78, 57, 35,  4,\n",
              "       71, 37, 61, 75, 72,  0, 91, 29, 47, 69,  7, 88, 29, 94, 55,  8, 61,\n",
              "       16, 49, 87, 57, 69, 13, 29, 60, 74, 26,  9, 39,  1, 62, 81, 14, 32,\n",
              "       90, 42, 60, 72, 40, 26, 84, 28, 37, 66, 29, 13, 25, 57, 44])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# View the prediction of the next character index at each time step.\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Print the input text for the first example in the first batch and the predicted text (next characters) based on the sampled indices from the untrained model’s output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcFwPwLSo05",
        "outputId": "653b6676-df44-4926-cc18-21e70a6b02fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'rking on it.\\nI deal with this stuff every day. But a fireman... you never get used to it.  What happ'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"JaQr4gqZ1vy\\t|KP)J25>R9|'d^'0dO\\nT+MP-N<tr\\n?vH1o;nXB!gD]kh[UNK]{;Ne$x;~V%].PwXe+;[j8&F\\t^q,>zI[hG8t:Db;+7XK\"\n"
          ]
        }
      ],
      "source": [
        "# Print the input text for the first example in the first batch.\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "\n",
        "print()\n",
        "\n",
        "# Print the predicted next character based on sampled indices.\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "The problem can now be treated as a standard classification task. Given the previous RNN state, and the input at this time step, we are required to predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxPU7pRtQ6QG"
      },
      "source": [
        "### Distributed training and parallelization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjRkDNsQi1N"
      },
      "source": [
        "As indicated earlier, we would leverage TensorFlow's distributed computing framework to optimize the use of computational resources and to minimize the time and cost required to train our model. Go ahead and set up TensorFlow to leverage TPU resources for distributed training.\n",
        "\n",
        "The `TPUClusterResolver` connects to the TPU system, initializes it, and verifies that the TPU resources are ready and properly configured before starting the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHybT-rDQuqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b032e5-1fd7-4412-a822-1af4c98ccd95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x79187827ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Initialize the TPU cluster resolver to connect to the TPU system.\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "# Connect to the TPU cluster using the resolver.\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "# Initialize the TPU system to prepare it for training.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7SaikTgQvhM"
      },
      "source": [
        "Define a strategy to distribute the training across TPU cores, aiming to accelerate and optimize the training process. It is worth noting that in this particular case, distributed computing using the `TPUStrategy` reduces the training time by a factor of hundreds, as compared to the time required to train the model without implementing parallelization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lme8nIznQ1zO"
      },
      "outputs": [],
      "source": [
        "# Step 2: Define the distribution strategy to distribute the training across the TPU cores.\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach a loss function and an optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The model needs a loss function and an optimizer for training. The standard `tf.keras.losses.sparse_categorical_crossentropy()` loss function is a good choice in this case. This is because it operates on the last dimension of the predictions, which contains the logits.\n",
        "\n",
        "Additionally, since the model returns logits, set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "# Define the loss function.\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the mean loss for the predictions of the first batch of data and confirm the shape of the prediction tensor."
      ],
      "metadata": {
        "id": "gvlbzfe8jRgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "8ef32f07-e3ed-48cf-bbb7-e8f38d7da98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (256, 100, 95)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.554161, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Calculate the mean loss for the predictions of the first batch of data.\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "# Print the shape of the prediction tensor: (batch_size, sequence_length, vocab_size).\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# Print the calculated mean loss for the predictions of the first batch of data.\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model should not be very confident in its predictions. For character prediction tasks, this means that the output logits should have similar magnitudes. If the model outputs very high or very low values for certain characters, it could signal poor initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check for any irregularities in the values of the output logits, confirm that the exponential of the mean loss is approximately equal to the vocabulary size. This is based on the idea that when the model is initialized with random weights, it should predict each character with roughly equal likelihood. In this case, the predicted probabilities for each character should be close to `1/vocabulary size`. The `SparseCategoricalCrossentropy()` loss function calculates the difference between the predicted probabilities and the true labels. Therefore, taking the exponential of the mean loss approximates the inverse of the vocabulary size, as cross-entropy loss is related to the negative log-likelihood. If the mean loss is much higher than this value, it indicates that the model is overly confident in its wrong predictions, suggesting that the model is badly initialized.\n",
        "\n",
        "Go ahead and compute the exponential of the mean loss to approximate the predicted probability distribution."
      ],
      "metadata": {
        "id": "M2Cd36D9bHbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAJfS5YoFiHf",
        "outputId": "5d0e7e2e-fa8b-4e9a-cce0-1486159c46c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.027"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Compute the exponential of the mean loss to approximate the predicted probability distribution.\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and train the final model"
      ],
      "metadata": {
        "id": "fvX0auLGZdxC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Since the test model performs as expected, go ahead and create the final / original model to be used for training. This final model will utilize the distributed training infrastructure provided by the TPU. This requires initializing and compiling the model within the scope of the TPU’s distributed training strategy - `TPUStrategy`. Configure the training procedure using the `tf.keras.Model.compile()` method to compile the model. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the scope for distributed TPU training using the \"TPUStrategy\".\n",
        "with strategy.scope():\n",
        "    # Initialize the \"RNNModel\" with specified vocabulary size, embedding dimension, and the number of RNN units.\n",
        "    model = RNNModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units\n",
        "    )\n",
        "\n",
        "    # Compile the model with Adam optimizer and the \"SparseCategoricalCrossentropy\" loss.\n",
        "    model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "AlDgz8mPz_K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint()` to ensure that checkpoints are saved during training. A checkpoint saves a model’s state (weights and optimizer) during training, allowing you to resume training or use the model later without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory to store the training checkpoints.\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "\n",
        "# Name of the checkpoint files.\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "# Create a callback to save the model weights during training.\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,  # Path to save the checkpoints.\n",
        "    save_weights_only=True  # Save only the model weights, not the entire model.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "Define the number of epochs to train the data. `30` should be a reasonable number for the model to learn effectively from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs to train the data.\n",
        "EPOCHS = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and fit the model."
      ],
      "metadata": {
        "id": "5sRnDiKncKOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "b8c0df5b-c09f-472f-a33e-a33f0726398d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "666/666 [==============================] - 43s 42ms/step - loss: 1.9709\n",
            "Epoch 2/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.3789\n",
            "Epoch 3/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.2923\n",
            "Epoch 4/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.2516\n",
            "Epoch 5/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.2254\n",
            "Epoch 6/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.2058\n",
            "Epoch 7/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1901\n",
            "Epoch 8/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1770\n",
            "Epoch 9/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1658\n",
            "Epoch 10/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1559\n",
            "Epoch 11/30\n",
            "666/666 [==============================] - 28s 41ms/step - loss: 1.1475\n",
            "Epoch 12/30\n",
            "666/666 [==============================] - 28s 41ms/step - loss: 1.1403\n",
            "Epoch 13/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1337\n",
            "Epoch 14/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1281\n",
            "Epoch 15/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1234\n",
            "Epoch 16/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1195\n",
            "Epoch 17/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1162\n",
            "Epoch 18/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1132\n",
            "Epoch 19/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1108\n",
            "Epoch 20/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1090\n",
            "Epoch 21/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1075\n",
            "Epoch 22/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1060\n",
            "Epoch 23/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1052\n",
            "Epoch 24/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1042\n",
            "Epoch 25/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1036\n",
            "Epoch 26/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1032\n",
            "Epoch 27/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1030\n",
            "Epoch 28/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1027\n",
            "Epoch 29/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1027\n",
            "Epoch 30/30\n",
            "666/666 [==============================] - 28s 42ms/step - loss: 1.1030\n"
          ]
        }
      ],
      "source": [
        "# Train and fit the model.\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The goal of this work is to perform character-level prediction using an RNN and leverage that capability to generate text sequences. The simplest way to generate text with this model is to run it in a loop while maintaining and updating the model’s internal state at each time step. The image below illustrates the text generation process using sampling from the trained model.\n",
        "\n",
        "**Note:** The image has been sourced from the original TensorFlow tutorial about [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input text and an internal state is passed to the model each time it is called. The model outputs a prediction for the next character along with an updated state. The prediction and state are then fed back into the model to continue generating text."
      ],
      "metadata": {
        "id": "4i19yT8dc-HS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The `OneStep` class makes a single step prediction, i.e., generates one text character at each time step, taking into account the previous characters and the model state, and then predicting the next character. It takes an `RNNModel` and converts token IDs to characters and vice versa. The class handles sampling from the model’s predicted logits, applying `temperature` to control randomness, and ensuring that the `[UNK]` token is not selected during generation. The `generate_one_step()` method performs the core text generation by splitting input text into characters, running the model to predict the next character, and converting the output token IDs back into characters. It also returns the updated states, which are essential for generating subsequent characters in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** To enhance performance, enable faster execution, and improve compatibility with TensorFlow, use the `@tf.function` decorator on the following function. Unlike eager execution, which processes operations step-by-step, this decorator converts the function into a TensorFlow graph, which is optimized and executed as a whole. Learn [more](https://www.tensorflow.org/guide/intro_to_graphs)."
      ],
      "metadata": {
        "id": "XX26a9V5J4vt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature  # \"temperature\" parameter to control the randomness.\n",
        "    self.model = model  # \"RNNModel\" for text generation.\n",
        "    self.chars_from_ids = chars_from_ids  # Converts token IDs to characters.\n",
        "    self.ids_from_chars = ids_from_chars  # Converts characters to token IDs.\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]  # Token ID for \"[UNK]\".\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float(\"inf\")]*len(skip_ids),  # Set \"-inf\" for the skipped token IDs.\n",
        "        indices=skip_ids,  # Indices of the skipped token IDs.\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])  # Match vocabulary size.\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)  # Convert sparse tensor to dense.\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")  # Split the input text into characters.\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()  # Map the characters to token IDs.\n",
        "\n",
        "    # Run the model with the input token IDs and the previous state, returning both the logits and the updated states.\n",
        "    # \"predicted_logits.shape\" is [batch, char, next_char_logits].\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]  # Get the logits for the last time step.\n",
        "    predicted_logits = predicted_logits/self.temperature  # Adjust the logits by \"temperature\".\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)  # Sample token IDs.\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)  # Remove the unnecessary dimensions.\n",
        "\n",
        "    # Convert from token IDs to characters.\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and the model states.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the `OneStep` model with the trained RNN-based model and the functions to convert token IDs to characters and vice versa."
      ],
      "metadata": {
        "id": "AI-rixkUu3gH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "# Initialize the \"OneStep\" model with the trained RNN-based model and the functions to convert token IDs to characters and vice versa.\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run the `OneStep` model in a loop to generate some text. Upon investigating the generated text, it is clear that the RNN-based model understands when to capitalize, imitates dialogue-like interactions, and uses a dialogue-like writing vocabulary. However, due to the small number of training epochs, it has not yet learned to form coherent sentences or conversations and produces some misspelled words. Therefore, the model would benefit from further training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "d866fbf4-230d-44f0-dce7-91dfed90bbed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They would'a wanna know where we can do. Maybe you could stay put, much.  Real life.\n",
            "\n",
            "I'm not the funny thing.\n",
            "Believe me.\n",
            "\n",
            "I mean with a person who said something so well come up with.\n",
            "Push it nearer. Miss...\n",
            "\n",
            "I knew you wouldn't be wrong.\n",
            "You have the money to get to go is under part of running store for a coupon? Can you hear that state sensitive?\n",
            "Yes...\n",
            "So where the hell you until Friday or Porsi'?\n",
            "I think Sidney Voicecook --\n",
            "I know.  I got the question a thing.  Take your time and watch humb. We were fine.\n",
            "\n",
            "You must be good. For the debrasks in the tent year old.\n",
            "\n",
            "What ha ha he wants?\n",
            "Never.\n",
            "\n",
            "Barnegin?\n",
            "A revenger?\n",
            "Aw, Sound. For the truth.  Dumple!  Stript! Now don't mentime rules in December 195's... you have to <u>have a problem with anything available impression.\n",
            "\n",
            "This is mine.\n",
            "Fourth. Yor ReniTwens everything income, didn't it?\n",
            "\n",
            "If you pay me that way, we'll start your courage.\n",
            "Don't be fishing?\n",
            "\n",
            "Can I finish with your report tomorrow?\n",
            "Private?\n",
            "I see, very. Why is this the reason  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 6.284204483032227\n"
          ]
        }
      ],
      "source": [
        "# Start the timer to measure the execution time.\n",
        "start = time.time()\n",
        "\n",
        "# Initialize the states to \"None\" (no initial state for the first prediction).\n",
        "states = None\n",
        "\n",
        "# Set the initial input text for the generation (starting word).\n",
        "next_char = tf.constant([\"They\"])\n",
        "\n",
        "# Initialize the list of results with the starting word.\n",
        "result = [next_char]\n",
        "\n",
        "# Loop to generate text for 1000 characters.\n",
        "for n in range(1000):\n",
        "    # Generate the next character and update the state.\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "\n",
        "    # Append the generated character to the list of results.\n",
        "    result.append(next_char)\n",
        "\n",
        "# Join all the characters in the list of results into one string.\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# End the timer to measure the execution time.\n",
        "end = time.time()\n",
        "\n",
        "# Print the generated text and add a separator line.\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\"*80)\n",
        "\n",
        "# Print the time taken to generate the text.\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "To generate the text faster, consider batching the text generation process. For instance, in the example below, the model generates five outputs simultaneously, taking roughly the same amount of time as generating a single output in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLu7Y8UCMT7",
        "outputId": "69dbd3c8-3832-4210-a551-5b83647c12aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'They seem to have stood the burglar site, try it again...wanna be a real jewelry of the drug.\\nWhat\\'s any of this in you?\\n\\nCan you get that thing like this...\\nJust answer me, Sir- dead Jack.  Or My Back. It makes more things -- I was a dirty lounger.  These other fighters.\\nThere is it anyway.  How do you like it?\\n\\nHow do you know her?\\nNothin\\' colors...  Just come around.\\nSorry about.\\n\\nNo. It\\'s like the damage of anybrook.\\nHi, sweetie.\\nLOOK. Was it all right.  Hello, more here.\\nThat\\'s why he told me before--chocks!  I\\'d like to swing it without you!\\nNaperful!  We\\'ll slap you out!  Typowers, Moscow will let me in on what causes propect -- what if I don\\'t care - And listen?  I got big months this evening as we pay $5! -- and it\\'s a business health in the garden.  Just like you said, \"Brad...he\\'s I returned your home.\" Just somebody a shoe... You\\'re going to deflote my father.\\nMicheal, I\\'m terrified.  When I had no bad choices you were dead, Esquid, Mr. Towler.  I just wish he was putting in He'\n",
            " b\"They can take you together.\\nLike making a fun. I...I haven't figured you.\\n\\nI knew it.\\nI guess you ignored it from something about dad and you're dead, dear by, you don't law enough for ya.\\n\\nLois?  Yes?\\nThey'll drop the guns. To get her to doy?\\n\\nHow would you like to introduce me unpaposible line?\\nI knew you were.  Got written by things before they had a full head on his pine-go.  He was dead.  I just don't know if you did, but through this and medicin process. If you can find him, I would have done it.\\n\\nYeah?\\nWhy the hell it looks too aboard.\\n\\nMichael.\\n\\n... he wasn't missate.\\n\\nWhat are you, amazing, Gloria.  Your neighbors Imelse is, the eastern.  What am I supposed to do with women?  I would, nose and days at all, all three hours - do me the lesser!\\n\\nI apologize for the sound of workings live long. We had a better idea where can he work. She's dead.  Two million.  Sit down this way -- the starboard son of all his wife had weared me before and I accept this man that shoots you in.  Ask in,\"\n",
            " b\"They're going to rime a ringle good time.\\nYou know, you said you were a brainer, or he would be an oxygen too.  Goodbye.\\n\\nI'm well.......I'll try.\\nFly Christmas.\\nOh friend, not a word shop in is...  .\\n\\nLet's see if there's a full neat. He hasn't catch him fifty our bodies off the living death.\\nWhat else is the work like that, why she used to say to be so much from others. Bobby and Fran will never get with him.\\nJesus, look, same good sweat it a deal.\\nThanks.\\nGoodnight.\\n\\nGoodbye?\\n\\nAnger!  Activate Union won't tell us!\\nTell me they were going up to find the minute.  The whole country parents was born.  <u>You</u> know, where the hell I'm coming Di. And I thought it was forty seconds to feel like you will.\\nWait, wait, wait, wait 'em pull.\\n\\nWell, whatever you want.\\nWell, for one, now.\\nWhite and a must stomach.\\nHere's so funny, I think we find out a lot all alone.  Just want to cry I'm talking.\\n\\nWhat's that?  What the hell is going on after he's behind?\\nI'm your past.  All he did.\\n\\nTed.  Who se\"\n",
            " b'They\\'re cute, Tony.\\n\\nLet me just go pain.\\nI write for the pentholest thing in the next stream.\\nYeah. Well, that\\'ll be \\'Kelloma\\'s Park. He\\'s been here.  But we\\'re repiring and he spoke to anyone like them.\\nWhat telepantly has intempting his checks?\\nThe thoughts have broken.  Will my wife fucking leaving the stick sons with another of the Frenkine, Nyma?\\n\\nUh, Scratch...the whole world detected nobody ever took Overdo... these my call.\" Then we\\'re dangerous.  Glon.\\nListen to me for three!\\nAre you saying that a guy\\'s got to be a movie star, he was starting with me on the enormour awhile.\\nI think it\\'s difficult for you.\\n\\nCome on the court, buriod.  Stay or nothing.\\nSo?\\nYes...\\nWell, it\\'s just me. I\\'m sorry.  I called Ericas and every gossible at the street Jane. And you will be killed. About my brother.  But we need the money. Must we have that dumper into legit last night again, and they all feel terrible full at the grade department, did my capal?\\nWell, she\\'s a fellow. Where it was --\\nBut let '\n",
            " b\"They don't build a scientiation--what the fuck are we doing like that, as the angles and shit persons, Bertranding, I'm a Tibian- optimist, Erich, Frederiak, of course!\\nThen-- for worth distingting anto, man! One mind even requires cloud. She idiots!  Since I'm feeling new in the cameras, I'll turn in the Airlines; it'll be fine - Will I get down to left smart that psyched the thing?\\nI'd say my guest is overrated with your product on your sperm.  You're Lecktor, hello Reginal, really.\\nWhere?\\nThe Olds and Ginback and willspires saying we talked about a few six percent. White, no propogate bomb with break in the back of his cerlean.\\nI did.\\n\\nUpham, that would still be out of...\\n...general answers, or doesnt sculture me in a scars we made love. A cop question with Dorok Futh, swords:  you sent me here time to get out!  You're giving it real any man you want me.\\nWhat are you doing tonight.\\nI heard much use a hombshit to love me if I had somebody didn't come back in serious business, I was here!\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 6.450711011886597\n"
          ]
        }
      ],
      "source": [
        "# Start the timer to measure the execution time.\n",
        "start = time.time()\n",
        "\n",
        "# Initialize the states to \"None\" (no initial state for the first prediction).\n",
        "states = None\n",
        "\n",
        "# Batch the text generation by providing multiple starting words at once.\n",
        "next_char = tf.constant([\"They\", \"They\", \"They\", \"They\", \"They\"])\n",
        "\n",
        "# Initialize the list of results with the starting word.\n",
        "result = [next_char]\n",
        "\n",
        "# Loop to generate text for 1000 characters.\n",
        "for n in range(1000):\n",
        "    # Generate the next character and update the state.\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "\n",
        "    # Append the generated character to the list of results.\n",
        "    result.append(next_char)\n",
        "\n",
        "# Join all the characters in the list of results into one string.\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# End the timer to measure the execution time.\n",
        "end = time.time()\n",
        "\n",
        "# Print the entire generated text in the result, followed by a separator line to improve readability.\n",
        "print(result, \"\\n\\n\" + \"_\"*80)\n",
        "\n",
        "# Print the time taken to generate the text.\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Save](https://www.tensorflow.org/guide/saved_model) the single-step model and restore and use it anywhere a `tf.saved_model` is accepted."
      ],
      "metadata": {
        "id": "DpVMD-sETywA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Grk32H_CzsC",
        "outputId": "b8500295-a310-4a3c-ba9f-b1f22500c004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x791408772740>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "# Save the \"OneStep\" model to the specified directory.\n",
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "\n",
        "# Reload the saved \"OneStep\" model from the specified directory.\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z9bb_wX6Uuu",
        "outputId": "17fe0547-2acb-4440-c995-75b17813078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They want us to have whacks.\n",
            "Guess what.  Let me see if your chunk is not--\n",
            "Mon has it work. If you don'\n"
          ]
        }
      ],
      "source": [
        "# Initialize the states to \"None\" (no initial state for the first prediction).\n",
        "states = None\n",
        "\n",
        "# Set the initial input text for the generation (starting word).\n",
        "next_char = tf.constant([\"They\"])\n",
        "\n",
        "# Initialize the list of results with the starting word.\n",
        "result = [next_char]\n",
        "\n",
        "# Loop to generate text for 100 characters.\n",
        "for n in range(100):\n",
        "  # Generate the next character and update the state.\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "\n",
        "  # Append the generated character to the list of results.\n",
        "  result.append(next_char)\n",
        "\n",
        "# Join all the characters in the list of results into one string.\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the generated text.\n",
        "print(result[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thoughts"
      ],
      "metadata": {
        "id": "mGEVqU8Gbjv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "We have successfully trained an RNN-based sequence model to perform character-level prediction for text generation. However, a majority of the generated sentences are not grammatically correct or coherent, though some of them are. The model has not learned the meaning of words and does tend to misspell words, but consider the following.\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* With the small number of training epochs, the model has not yet mastered forming coherent sentences or conversations and would benefit from additional training.\n",
        "\n",
        "* Barring a few exceptions, the model knows when to capitalize text.\n",
        "\n",
        "* Similar to the dataset, the structure of the output resembles conversations with dialogue-like text.\n",
        "\n",
        "* Even when the model is trained on small batches of text (100 characters each), it is capable of generating longer, structured sequences of text with some degree of coherence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps"
      ],
      "metadata": {
        "id": "yQ9QUsGa7-da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, there is a definite scope for improvement in this work. There are multiple ways to further develop this dirty implementation and improve the quality and coherence of text generated by the RNN-based model. Some of those are listed below.\n",
        "\n",
        "1.   **Train for Longer:** Increasing the number of epochs (e.g., try setting `EPOCHS = 50`) for training allows the model to learn more patterns and better representations, but be mindful of overfitting.\n",
        "2.   **Experiment with Different Start Strings:** Trying different starting strings could yield interesting variations and better contextual relevance in the text.\n",
        "3.   **Modify the Architecture:** Adding more RNN layers - GRUs or other types of RNN layers, increasing the number of RNN / GRU units, or replacing the GRU with Long Short-Term Memory (LSTM) networks can enable the model to learn more complex patterns. Similarly, incorporating more dense (fully connected) layers can enhance the model’s capacity to capture intricate relationships. However, to prevent overfitting and manage training time and computational costs, simplifying the model architecture, rather than adding more layers, may be a more effective approach.\n",
        "4.   **Adjust `temperature`:** Adjusting the `temperature` parameter can help increase or decrease the randomness of predictions.\n",
        "5.   **Hyperparameter Tuning:** Experimenting with different learning rates, batch sizes, epochs, etc. can help.\n",
        "6.   **Customized Training:** This tutorial adopts a simple training procedure and does not give you enough control. It uses teacher-forcing, which prevents bad predictions from being fed back to the model, so the model never learns to recover from its mistakes. Review TensorFlow's original tutorial about [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation) to learn more about implementing a custom training loop."
      ],
      "metadata": {
        "id": "Mdj31zDb86ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You may be prompted to restart the TPU after uninstalling and reinstalling NumPy. If prompted, proceed with restarting the TPU."
      ],
      "metadata": {
        "id": "hI6TW349O618"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}